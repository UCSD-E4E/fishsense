@Article{Wang2022,
  author        = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  title         = {YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
  year          = {2022},
  month         = jul,
  abstract      = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed and 2% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/WongKinYiu/yolov7.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2207.02696},
  eprint        = {2207.02696},
  file          = {:http\://arxiv.org/pdf/2207.02696v1:PDF},
  groups        = {Fish Analysis},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Knausgaard2021,
  author    = {Kristian Muri Knausg{\aa}rd and Arne Wiklund and Tonje Knutsen S{\o}rdalen and Kim Tallaksen Halvorsen and Alf Ring Kleiven and Lei Jiao and Morten Goodwin},
  journal   = {Applied Intelligence},
  title     = {Temperate fish detection and classification: a deep learning based approach},
  year      = {2021},
  month     = {mar},
  number    = {6},
  pages     = {6988--7001},
  volume    = {52},
  doi       = {10.1007/s10489-020-02154-9},
  groups    = {Fish Analysis},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Cisar2021,
  author    = {Petr Cisar and Dinara Bekkozhayeva and Oleksandr Movchan and Mohammadmehdi Saberioon and Rudolf Schraml},
  journal   = {Scientific Reports},
  title     = {Computer vision based individual fish identification using skin dot pattern},
  year      = {2021},
  month     = {aug},
  number    = {1},
  volume    = {11},
  doi       = {10.1038/s41598-021-96476-4},
  file      = {:https\://www.nature.com/articles/s41598-021-96476-4.pdf:PDF},
  groups    = {Fish Analysis},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Tueller2021,
  author    = {Peter Tueller and Raghav Maddukuri and Patrick Paxson and Vivaswat Suresh and Arjun Ashok and Madison Bland and Ronan Wallace and Julia Guerrero and Brice Semmens and Ryan Kastner},
  booktitle = {{OCEANS} 2021: San Diego {\textendash} Porto},
  title     = {{FishSense}: Underwater {RGBD} Imaging for Fish Measurement},
  year      = {2021},
  month     = {sep},
  publisher = {{IEEE}},
  doi       = {10.23919/oceans44145.2021.9705929},
  groups    = {RealSense System},
}

@Article{Lavest2003,
  author    = {J.M. Lavest and G. Rives and J.T. Laprest�},
  journal   = {Machine Vision and Applications},
  title     = {Dry camera calibration for underwater applications},
  year      = {2003},
  month     = {mar},
  number    = {5-6},
  pages     = {245--253},
  volume    = {13},
  doi       = {10.1007/s00138-002-0112-z},
  groups    = {Camera Calibration},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Hamid2022,
  author    = {Mohd Saad Hamid and NurulFajar Abd Manap and Rostam Affendi Hamzah and Ahmad Fauzan Kadmin},
  journal   = {Journal of King Saud University - Computer and Information Sciences},
  title     = {Stereo matching algorithm based on deep learning: A survey},
  year      = {2022},
  month     = {may},
  number    = {5},
  pages     = {1663--1673},
  volume    = {34},
  doi       = {10.1016/j.jksuci.2020.08.011},
  groups    = {Stereo Camera},
  publisher = {Elsevier {BV}},
}

@Article{Zhou2020,
  author    = {Kun Zhou and Xiangxi Meng and Bo Cheng},
  journal   = {Computational Intelligence and Neuroscience},
  title     = {Review of Stereo Matching Algorithms Based on Deep Learning},
  year      = {2020},
  month     = {mar},
  pages     = {1--12},
  volume    = {2020},
  doi       = {10.1155/2020/8562323},
  groups    = {Stereo Camera},
  publisher = {Hindawi Limited},
}

@Article{Liang2021,
  author    = {Zhengfa Liang and Yulan Guo and Yiliu Feng and Wei Chen and Linbo Qiao and Li Zhou and Jianfeng Zhang and Hengzhu Liu},
  journal   = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  title     = {Stereo Matching Using Multi-Level Cost Volume and Multi-Scale Feature Constancy},
  year      = {2021},
  month     = {jan},
  number    = {1},
  pages     = {300--315},
  volume    = {43},
  doi       = {10.1109/tpami.2019.2928550},
  groups    = {Stereo Camera},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{Yau2013,
  author    = {Timothy Yau and Minglun Gong and Yee-Hong Yang},
  booktitle = {2013 {IEEE} Conference on Computer Vision and Pattern Recognition},
  title     = {Underwater Camera Calibration Using Wavelength Triangulation},
  year      = {2013},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/cvpr.2013.323},
  groups    = {Camera Calibration},
}

@Conference{Wong2022,
  author    = {Emily Wong and Isabella Humphrey and Scott Switzer and Christopher Crutchfield and Nathan Hui and Curt Schurgers and Ryan Kastner},
  booktitle = {Proceedings of the 16th International Conference on Underwater Networks & Systems},
  title     = {Underwater Depth Calibration Using a Commercial Depth Camera},
  year      = {2022},
  address   = {New York, NY, USA},
  month     = {12},
  pages     = {1–5},
  publisher = {Association for Computing Machinery},
  series    = {WUWNet '22},
  abstract  = {Depth cameras are increasingly used in research and industry in underwater settings. However, cameras that have been calibrated in air are notably inaccurate in depth measurements when placed underwater, and little research has been done to explore pre-existing depth calibration methodologies and their effectiveness in underwater environments. We used four methods of calibration on a low-cost, commercial depth camera both in and out of water. For each of these methods, we compared the predicted distance and length of objects from the camera with manually measured values to get an indication of depth and length accuracy. Our findings indicate that the standard methods of calibration in air are largely ineffective for underwater calibration and that custom calibration techniques are necessary to achieve higher accuracy.},
  day       = {29},
  doi       = {10.1145/3567600.3568158},
  groups    = {Camera Calibration, Stereo Camera, RealSense System},
  isbn      = {9781450399524},
  keywords  = {Underwater stereo vision, depth camera calibration},
  location  = {Boston, MA, USA},
  pagetotal = {5},
  url       = {https://doi.org/10.1145/3567600.3568158},
}

@InProceedings{Lu2019,
  author    = {Shuigen Lu and Hesheng Yin and Yunliang Zhu and Xi Yang and Shaomiao Li and Bo Huang},
  booktitle = {Proceedings of the 2019 4th International Conference on Robotics, Control and Automation},
  title     = {Binocular Stereo Matching Based on Convolutional Neural Networks},
  year      = {2019},
  month     = {jul},
  publisher = {{ACM}},
  doi       = {10.1145/3351180.3351189},
  groups    = {Stereo Camera},
}

@Article{Tonioni2018,
  author        = {Tonioni, Alessio and Tosi, Fabio and Poggi, Matteo and Mattoccia, Stefano and Di Stefano, Luigi},
  title         = {Real-time self-adaptive deep stereo},
  year          = {2018},
  month         = oct,
  abstract      = {Deep convolutional neural networks trained end-to-end are the state-of-the-art methods to regress dense disparity maps from stereo pairs. These models, however, suffer from a notable decrease in accuracy when exposed to scenarios significantly different from the training set, e.g., real vs synthetic images, etc.). We argue that it is extremely unlikely to gather enough samples to achieve effective training/tuning in any target domain, thus making this setup impractical for many applications. Instead, we propose to perform unsupervised and continuous online adaptation of a deep stereo network, which allows for preserving its accuracy in any environment. However, this strategy is extremely computationally demanding and thus prevents real-time inference. We address this issue introducing a new lightweight, yet effective, deep stereo architecture, Modularly ADaptive Network (MADNet) and developing a Modular ADaptation (MAD) algorithm, which independently trains sub-portions of the network. By deploying MADNet together with MAD we introduce the first real-time self-adaptive deep stereo system enabling competitive performance on heterogeneous datasets.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1810.05424},
  eprint        = {1810.05424},
  file          = {:http\://arxiv.org/pdf/1810.05424v2:PDF},
  groups        = {Stereo Camera},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Tulyakov2018,
  author        = {Tulyakov, Stepan and Ivanov, Anton and Fleuret, Francois},
  title         = {Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching},
  year          = {2018},
  month         = jun,
  abstract      = {End-to-end deep-learning networks recently demonstrated extremely good perfor- mance for stereo matching. However, existing networks are difficult to use for practical applications since (1) they are memory-hungry and unable to process even modest-size images, (2) they have to be trained for a given disparity range. The Practical Deep Stereo (PDS) network that we propose addresses both issues: First, its architecture relies on novel bottleneck modules that drastically reduce the memory footprint in inference, and additional design choices allow to handle greater image size during training. This results in a model that leverages large image context to resolve matching ambiguities. Second, a novel sub-pixel cross- entropy loss combined with a MAP estimator make this network less sensitive to ambiguous matches, and applicable to any disparity range without re-training. We compare PDS to state-of-the-art methods published over the recent months, and demonstrate its superior performance on FlyingThings3D and KITTI sets.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1806.01677},
  eprint        = {1806.01677},
  file          = {:http\://arxiv.org/pdf/1806.01677v1:PDF},
  groups        = {Stereo Camera},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Siddique2020,
  author        = {Siddique, Nahian and Sidike, Paheding and Elkin, Colin and Devabhaktuni, Vijay},
  journal       = {{IEEE} Access},
  title         = {U-Net and its variants for medical image segmentation: theory and applications},
  year          = {2020},
  month         = nov,
  pages         = {82031--82057},
  volume        = {9},
  abstract      = {U-net is an image segmentation technique developed primarily for medical image analysis that can precisely segment images using a scarce amount of training data. These traits provide U-net with a very high utility within the medical imaging community and have resulted in extensive adoption of U-net as the primary tool for segmentation tasks in medical imaging. The success of U-net is evident in its widespread use in all major image modalities from CT scans and MRI to X-rays and microscopy. Furthermore, while U-net is largely a segmentation tool, there have been instances of the use of U-net in other applications. As the potential of U-net is still increasing, in this review we look at the various developments that have been made in the U-net architecture and provide observations on recent trends. We examine the various innovations that have been made in deep learning and discuss how these tools facilitate U-net. Furthermore, we look at image modalities and application areas where U-net has been applied.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.1109/access.2021.3086020},
  eprint        = {2011.01118},
  file          = {:http\://arxiv.org/pdf/2011.01118v1:PDF},
  groups        = {Fish Analysis},
  keywords      = {Image and Video Processing (eess.IV), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences},
  primaryclass  = {eess.IV},
  publisher     = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Yang2022,
  author    = {Hongbo Yang and Zhizun Xu and Baozhu Jia},
  journal   = {Sensors},
  title     = {An Underwater Positioning System for {UUVs} Based on {LiDAR} Camera and Inertial Measurement Unit},
  year      = {2022},
  month     = {jul},
  number    = {14},
  pages     = {5418},
  volume    = {22},
  doi       = {10.3390/s22145418},
  groups    = {Underwater Positioning},
  publisher = {{MDPI} {AG}},
}

@InProceedings{Agrawal2012,
  author    = {Amit Agrawal and Srikumar Ramalingam and Yuichi Taguchi and Visesh Chari},
  booktitle = {2012 {IEEE} Conference on Computer Vision and Pattern Recognition},
  title     = {A theory of multi-layer flat refractive geometry},
  year      = {2012},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/cvpr.2012.6248073},
  groups    = {Optics},
}

@InProceedings{Yang2020,
  author    = {Menglong Yang and Fangrui Wu and Wei Li},
  booktitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  title     = {{WaveletStereo}: Learning Wavelet Coefficients of Disparity Map in Stereo Matching},
  year      = {2020},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/cvpr42600.2020.01290},
  groups    = {Stereo Camera},
}

@InProceedings{Dawkins2017,
  author    = {Matthew Dawkins and Linus Sherrill and Keith Fieldhouse and Anthony Hoogs and Benjamin Richards and David Zhang and Lakshman Prasad and Kresimir Williams and Nathan Lauffenburger and Gaoang Wang},
  booktitle = {2017 {IEEE} Winter Conference on Applications of Computer Vision ({WACV})},
  title     = {An Open-Source Platform for Underwater Image and Video Analytics},
  year      = {2017},
  month     = {mar},
  publisher = {{IEEE}},
  doi       = {10.1109/wacv.2017.105},
  groups    = {Fish Analysis},
}

@Article{Luczynski2017,
  author    = {Tomasz {\L}uczy{\'{n}}ski and Max Pfingsthorn and Andreas Birk},
  journal   = {Ocean Engineering},
  title     = {The Pinax-model for accurate and efficient refraction correction of underwater cameras in flat-pane housings},
  year      = {2017},
  month     = {mar},
  pages     = {9--22},
  volume    = {133},
  doi       = {10.1016/j.oceaneng.2017.01.029},
  groups    = {Optics},
  publisher = {Elsevier {BV}},
}

@InProceedings{Gedge2011,
  author    = {Jason Gedge and Minglun Gong and Yee-Hong Yang},
  booktitle = {2011 Canadian Conference on Computer and Robot Vision},
  title     = {Refractive Epipolar Geometry for Underwater Stereo Matching},
  year      = {2011},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/crv.2011.26},
  groups    = {Stereo Camera, Optics},
}

@Article{Zhang2020,
  author        = {Zhang, Jingyang and Yao, Yao and Luo, Zixin and Li, Shiwei and Shen, Tianwei and Fang, Tian and Quan, Long},
  title         = {Learning Stereo Matchability in Disparity Regression Networks},
  year          = {2020},
  month         = aug,
  abstract      = {Learning-based stereo matching has recently achieved promising results, yet still suffers difficulties in establishing reliable matches in weakly matchable regions that are textureless, non-Lambertian, or occluded. In this paper, we address this challenge by proposing a stereo matching network that considers pixel-wise matchability. Specifically, the network jointly regresses disparity and matchability maps from 3D probability volume through expectation and entropy operations. Next, a learned attenuation is applied as the robust loss function to alleviate the influence of weakly matchable pixels in the training. Finally, a matchability-aware disparity refinement is introduced to improve the depth inference in weakly matchable regions. The proposed deep stereo matchability (DSM) framework can improve the matching result or accelerate the computation while still guaranteeing the quality. Moreover, the DSM framework is portable to many recent stereo networks. Extensive experiments are conducted on Scene Flow and KITTI stereo datasets to demonstrate the effectiveness of the proposed framework over the state-of-the-art learning-based stereo methods.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2008.04800},
  eprint        = {2008.04800},
  file          = {:http\://arxiv.org/pdf/2008.04800v1:PDF},
  groups        = {Stereo Camera},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Zhou2010,
  author    = {Yu Zhou},
  journal   = {Robotica},
  title     = {A closed-form algorithm for the least-squares trilateration problem},
  year      = {2010},
  month     = {may},
  number    = {3},
  pages     = {375--389},
  volume    = {29},
  doi       = {10.1017/s0263574710000196},
  publisher = {Cambridge University Press ({CUP})},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Camera Calibration\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Fish Analysis\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Optics\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:RealSense System\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Stereo Camera\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Underwater Positioning\;0\;1\;0x8a8a8aff\;\;\;;
}
