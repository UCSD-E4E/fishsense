@Article{Wang2022,
  author        = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  title         = {YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
  year          = {2022},
  month         = jul,
  abstract      = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed and 2% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/WongKinYiu/yolov7.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2207.02696},
  eprint        = {2207.02696},
  file          = {:http\://arxiv.org/pdf/2207.02696v1:PDF},
  groups        = {Fish Analysis},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Knausgaard2021,
  author    = {Kristian Muri Knausg{\aa}rd and Arne Wiklund and Tonje Knutsen S{\o}rdalen and Kim Tallaksen Halvorsen and Alf Ring Kleiven and Lei Jiao and Morten Goodwin},
  journal   = {Applied Intelligence},
  title     = {Temperate fish detection and classification: a deep learning based approach},
  year      = {2021},
  month     = {mar},
  number    = {6},
  pages     = {6988--7001},
  volume    = {52},
  doi       = {10.1007/s10489-020-02154-9},
  groups    = {Fish Analysis},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Cisar2021,
  author    = {Petr Cisar and Dinara Bekkozhayeva and Oleksandr Movchan and Mohammadmehdi Saberioon and Rudolf Schraml},
  journal   = {Scientific Reports},
  title     = {Computer vision based individual fish identification using skin dot pattern},
  year      = {2021},
  month     = {aug},
  number    = {1},
  volume    = {11},
  doi       = {10.1038/s41598-021-96476-4},
  file      = {:https\://www.nature.com/articles/s41598-021-96476-4.pdf:PDF},
  groups    = {Fish Analysis},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Tueller2021,
  author    = {Peter Tueller and Raghav Maddukuri and Patrick Paxson and Vivaswat Suresh and Arjun Ashok and Madison Bland and Ronan Wallace and Julia Guerrero and Brice Semmens and Ryan Kastner},
  booktitle = {{OCEANS} 2021: San Diego {\textendash} Porto},
  title     = {{FishSense}: Underwater {RGBD} Imaging for Fish Measurement},
  year      = {2021},
  month     = {sep},
  publisher = {{IEEE}},
  doi       = {10.23919/oceans44145.2021.9705929},
  groups    = {RealSense System},
}

@Article{Lavest2003,
  author    = {J.M. Lavest and G. Rives and J.T. Laprest�},
  journal   = {Machine Vision and Applications},
  title     = {Dry camera calibration for underwater applications},
  year      = {2003},
  month     = {mar},
  number    = {5-6},
  pages     = {245--253},
  volume    = {13},
  doi       = {10.1007/s00138-002-0112-z},
  groups    = {Camera Calibration},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Hamid2022,
  author    = {Mohd Saad Hamid and NurulFajar Abd Manap and Rostam Affendi Hamzah and Ahmad Fauzan Kadmin},
  journal   = {Journal of King Saud University - Computer and Information Sciences},
  title     = {Stereo matching algorithm based on deep learning: A survey},
  year      = {2022},
  month     = {may},
  number    = {5},
  pages     = {1663--1673},
  volume    = {34},
  doi       = {10.1016/j.jksuci.2020.08.011},
  groups    = {Stereo Camera},
  publisher = {Elsevier {BV}},
}

@Article{Zhou2020,
  author    = {Kun Zhou and Xiangxi Meng and Bo Cheng},
  journal   = {Computational Intelligence and Neuroscience},
  title     = {Review of Stereo Matching Algorithms Based on Deep Learning},
  year      = {2020},
  month     = {mar},
  pages     = {1--12},
  volume    = {2020},
  doi       = {10.1155/2020/8562323},
  groups    = {Stereo Camera},
  publisher = {Hindawi Limited},
}

@Article{Liang2021,
  author    = {Zhengfa Liang and Yulan Guo and Yiliu Feng and Wei Chen and Linbo Qiao and Li Zhou and Jianfeng Zhang and Hengzhu Liu},
  journal   = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  title     = {Stereo Matching Using Multi-Level Cost Volume and Multi-Scale Feature Constancy},
  year      = {2021},
  month     = {jan},
  number    = {1},
  pages     = {300--315},
  volume    = {43},
  doi       = {10.1109/tpami.2019.2928550},
  groups    = {Stereo Camera},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{Yau2013,
  author    = {Timothy Yau and Minglun Gong and Yee-Hong Yang},
  booktitle = {2013 {IEEE} Conference on Computer Vision and Pattern Recognition},
  title     = {Underwater Camera Calibration Using Wavelength Triangulation},
  year      = {2013},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/cvpr.2013.323},
  groups    = {Camera Calibration, Optics},
}

@Conference{Wong2022,
  author    = {Emily Wong and Isabella Humphrey and Scott Switzer and Christopher Crutchfield and Nathan Hui and Curt Schurgers and Ryan Kastner},
  booktitle = {Proceedings of the 16th International Conference on Underwater Networks & Systems},
  title     = {Underwater Depth Calibration Using a Commercial Depth Camera},
  year      = {2022},
  address   = {New York, NY, USA},
  month     = {12},
  pages     = {1–5},
  publisher = {Association for Computing Machinery},
  series    = {WUWNet '22},
  abstract  = {Depth cameras are increasingly used in research and industry in underwater settings. However, cameras that have been calibrated in air are notably inaccurate in depth measurements when placed underwater, and little research has been done to explore pre-existing depth calibration methodologies and their effectiveness in underwater environments. We used four methods of calibration on a low-cost, commercial depth camera both in and out of water. For each of these methods, we compared the predicted distance and length of objects from the camera with manually measured values to get an indication of depth and length accuracy. Our findings indicate that the standard methods of calibration in air are largely ineffective for underwater calibration and that custom calibration techniques are necessary to achieve higher accuracy.},
  day       = {29},
  doi       = {10.1145/3567600.3568158},
  groups    = {Camera Calibration, Stereo Camera, RealSense System},
  isbn      = {9781450399524},
  keywords  = {Underwater stereo vision, depth camera calibration},
  location  = {Boston, MA, USA},
  pagetotal = {5},
  url       = {https://doi.org/10.1145/3567600.3568158},
}

@InProceedings{Lu2019,
  author    = {Shuigen Lu and Hesheng Yin and Yunliang Zhu and Xi Yang and Shaomiao Li and Bo Huang},
  booktitle = {Proceedings of the 2019 4th International Conference on Robotics, Control and Automation},
  title     = {Binocular Stereo Matching Based on Convolutional Neural Networks},
  year      = {2019},
  month     = {jul},
  publisher = {{ACM}},
  doi       = {10.1145/3351180.3351189},
  groups    = {Stereo Camera},
}

@Article{Tonioni2018,
  author        = {Tonioni, Alessio and Tosi, Fabio and Poggi, Matteo and Mattoccia, Stefano and Di Stefano, Luigi},
  title         = {Real-time self-adaptive deep stereo},
  year          = {2018},
  month         = oct,
  abstract      = {Deep convolutional neural networks trained end-to-end are the state-of-the-art methods to regress dense disparity maps from stereo pairs. These models, however, suffer from a notable decrease in accuracy when exposed to scenarios significantly different from the training set, e.g., real vs synthetic images, etc.). We argue that it is extremely unlikely to gather enough samples to achieve effective training/tuning in any target domain, thus making this setup impractical for many applications. Instead, we propose to perform unsupervised and continuous online adaptation of a deep stereo network, which allows for preserving its accuracy in any environment. However, this strategy is extremely computationally demanding and thus prevents real-time inference. We address this issue introducing a new lightweight, yet effective, deep stereo architecture, Modularly ADaptive Network (MADNet) and developing a Modular ADaptation (MAD) algorithm, which independently trains sub-portions of the network. By deploying MADNet together with MAD we introduce the first real-time self-adaptive deep stereo system enabling competitive performance on heterogeneous datasets.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1810.05424},
  eprint        = {1810.05424},
  file          = {:http\://arxiv.org/pdf/1810.05424v2:PDF},
  groups        = {Stereo Camera},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Tulyakov2018,
  author        = {Tulyakov, Stepan and Ivanov, Anton and Fleuret, Francois},
  title         = {Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching},
  year          = {2018},
  month         = jun,
  abstract      = {End-to-end deep-learning networks recently demonstrated extremely good perfor- mance for stereo matching. However, existing networks are difficult to use for practical applications since (1) they are memory-hungry and unable to process even modest-size images, (2) they have to be trained for a given disparity range. The Practical Deep Stereo (PDS) network that we propose addresses both issues: First, its architecture relies on novel bottleneck modules that drastically reduce the memory footprint in inference, and additional design choices allow to handle greater image size during training. This results in a model that leverages large image context to resolve matching ambiguities. Second, a novel sub-pixel cross- entropy loss combined with a MAP estimator make this network less sensitive to ambiguous matches, and applicable to any disparity range without re-training. We compare PDS to state-of-the-art methods published over the recent months, and demonstrate its superior performance on FlyingThings3D and KITTI sets.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1806.01677},
  eprint        = {1806.01677},
  file          = {:http\://arxiv.org/pdf/1806.01677v1:PDF},
  groups        = {Stereo Camera},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Siddique2020,
  author        = {Siddique, Nahian and Sidike, Paheding and Elkin, Colin and Devabhaktuni, Vijay},
  journal       = {{IEEE} Access},
  title         = {U-Net and its variants for medical image segmentation: theory and applications},
  year          = {2020},
  month         = nov,
  pages         = {82031--82057},
  volume        = {9},
  abstract      = {U-net is an image segmentation technique developed primarily for medical image analysis that can precisely segment images using a scarce amount of training data. These traits provide U-net with a very high utility within the medical imaging community and have resulted in extensive adoption of U-net as the primary tool for segmentation tasks in medical imaging. The success of U-net is evident in its widespread use in all major image modalities from CT scans and MRI to X-rays and microscopy. Furthermore, while U-net is largely a segmentation tool, there have been instances of the use of U-net in other applications. As the potential of U-net is still increasing, in this review we look at the various developments that have been made in the U-net architecture and provide observations on recent trends. We examine the various innovations that have been made in deep learning and discuss how these tools facilitate U-net. Furthermore, we look at image modalities and application areas where U-net has been applied.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.1109/access.2021.3086020},
  eprint        = {2011.01118},
  file          = {:http\://arxiv.org/pdf/2011.01118v1:PDF},
  groups        = {Fish Analysis},
  keywords      = {Image and Video Processing (eess.IV), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences},
  primaryclass  = {eess.IV},
  publisher     = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Yang2022,
  author    = {Hongbo Yang and Zhizun Xu and Baozhu Jia},
  journal   = {Sensors},
  title     = {An Underwater Positioning System for {UUVs} Based on {LiDAR} Camera and Inertial Measurement Unit},
  year      = {2022},
  month     = {jul},
  number    = {14},
  pages     = {5418},
  volume    = {22},
  doi       = {10.3390/s22145418},
  groups    = {Underwater Positioning},
  publisher = {{MDPI} {AG}},
}

@InProceedings{Agrawal2012,
  author    = {Amit Agrawal and Srikumar Ramalingam and Yuichi Taguchi and Visesh Chari},
  booktitle = {2012 {IEEE} Conference on Computer Vision and Pattern Recognition},
  title     = {A theory of multi-layer flat refractive geometry},
  year      = {2012},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/cvpr.2012.6248073},
  groups    = {Optics},
}

@InProceedings{Yang2020,
  author    = {Menglong Yang and Fangrui Wu and Wei Li},
  booktitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  title     = {{WaveletStereo}: Learning Wavelet Coefficients of Disparity Map in Stereo Matching},
  year      = {2020},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/cvpr42600.2020.01290},
  groups    = {Stereo Camera},
}

@InProceedings{Dawkins2017,
  author    = {Matthew Dawkins and Linus Sherrill and Keith Fieldhouse and Anthony Hoogs and Benjamin Richards and David Zhang and Lakshman Prasad and Kresimir Williams and Nathan Lauffenburger and Gaoang Wang},
  booktitle = {2017 {IEEE} Winter Conference on Applications of Computer Vision ({WACV})},
  title     = {An Open-Source Platform for Underwater Image and Video Analytics},
  year      = {2017},
  month     = {mar},
  publisher = {{IEEE}},
  doi       = {10.1109/wacv.2017.105},
  groups    = {Fish Analysis},
}

@Article{Luczynski2017,
  author    = {Tomasz {\L}uczy{\'{n}}ski and Max Pfingsthorn and Andreas Birk},
  journal   = {Ocean Engineering},
  title     = {The Pinax-model for accurate and efficient refraction correction of underwater cameras in flat-pane housings},
  year      = {2017},
  month     = {mar},
  pages     = {9--22},
  volume    = {133},
  doi       = {10.1016/j.oceaneng.2017.01.029},
  groups    = {Optics},
  publisher = {Elsevier {BV}},
}

@InProceedings{Gedge2011,
  author    = {Jason Gedge and Minglun Gong and Yee-Hong Yang},
  booktitle = {2011 Canadian Conference on Computer and Robot Vision},
  title     = {Refractive Epipolar Geometry for Underwater Stereo Matching},
  year      = {2011},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/crv.2011.26},
  groups    = {Stereo Camera, Optics},
}

@Article{Zhang2020,
  author        = {Zhang, Jingyang and Yao, Yao and Luo, Zixin and Li, Shiwei and Shen, Tianwei and Fang, Tian and Quan, Long},
  title         = {Learning Stereo Matchability in Disparity Regression Networks},
  year          = {2020},
  month         = aug,
  abstract      = {Learning-based stereo matching has recently achieved promising results, yet still suffers difficulties in establishing reliable matches in weakly matchable regions that are textureless, non-Lambertian, or occluded. In this paper, we address this challenge by proposing a stereo matching network that considers pixel-wise matchability. Specifically, the network jointly regresses disparity and matchability maps from 3D probability volume through expectation and entropy operations. Next, a learned attenuation is applied as the robust loss function to alleviate the influence of weakly matchable pixels in the training. Finally, a matchability-aware disparity refinement is introduced to improve the depth inference in weakly matchable regions. The proposed deep stereo matchability (DSM) framework can improve the matching result or accelerate the computation while still guaranteeing the quality. Moreover, the DSM framework is portable to many recent stereo networks. Extensive experiments are conducted on Scene Flow and KITTI stereo datasets to demonstrate the effectiveness of the proposed framework over the state-of-the-art learning-based stereo methods.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2008.04800},
  eprint        = {2008.04800},
  file          = {:http\://arxiv.org/pdf/2008.04800v1:PDF},
  groups        = {Stereo Camera},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Zhou2010,
  author    = {Yu Zhou},
  journal   = {Robotica},
  title     = {A closed-form algorithm for the least-squares trilateration problem},
  year      = {2010},
  month     = {may},
  number    = {3},
  pages     = {375--389},
  volume    = {29},
  doi       = {10.1017/s0263574710000196},
  groups    = {Stereo Camera},
  publisher = {Cambridge University Press ({CUP})},
}

@Article{Hu2023RefractivePR,
  author  = {Xiao Hu and F. Lauze and Kim Steenstrup Pedersen},
  journal = {International Journal of Computer Vision},
  title   = {Refractive Pose Refinement},
  year    = {2023},
  pages   = {1448-1476},
  volume  = {131},
  doi     = {10.1007/s11263-023-01763-4},
  groups  = {Stereo Camera, Optics},
}

@Article{Raistrick2023,
  author        = {Raistrick, Alexander and Lipson, Lahav and Ma, Zeyu and Mei, Lingjie and Wang, Mingzhe and Zuo, Yiming and Kayan, Karhan and Wen, Hongyu and Han, Beining and Wang, Yihan and Newell, Alejandro and Law, Hei and Goyal, Ankit and Yang, Kaiyu and Deng, Jia},
  title         = {Infinite Photorealistic Worlds using Procedural Generation},
  year          = {2023},
  month         = jun,
  abstract      = {We introduce Infinigen, a procedural generator of photorealistic 3D scenes of the natural world. Infinigen is entirely procedural: every asset, from shape to texture, is generated from scratch via randomized mathematical rules, using no external source and allowing infinite variation and composition. Infinigen offers broad coverage of objects and scenes in the natural world including plants, animals, terrains, and natural phenomena such as fire, cloud, rain, and snow. Infinigen can be used to generate unlimited, diverse training data for a wide range of computer vision tasks including object detection, semantic segmentation, optical flow, and 3D reconstruction. We expect Infinigen to be a useful resource for computer vision research and beyond. Please visit https://infinigen.org for videos, code and pre-generated data.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2306.09310},
  eprint        = {2306.09310},
  file          = {:http\://arxiv.org/pdf/2306.09310v2:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Boudhane2016,
  author    = {Mohcine Boudhane and Benayad Nsiri},
  journal   = {Journal of Visual Communication and Image Representation},
  title     = {Underwater image processing method for fish localization and detection in submarine environment},
  year      = {2016},
  month     = {aug},
  pages     = {226--238},
  volume    = {39},
  doi       = {10.1016/j.jvcir.2016.05.017},
  groups    = {Fish Analysis},
  publisher = {Elsevier {BV}},
}

@Article{Wang2023,
  author        = {Wang, Weihan and Joshi, Bharat and Burgdorfer, Nathaniel and Batsos, Konstantinos and Li, Alberto Quattrini and Mordohai, Philippos and Rekleitis, Ioannis},
  title         = {Real-Time Dense 3D Mapping of Underwater Environments},
  year          = {2023},
  month         = apr,
  abstract      = {This paper addresses real-time dense 3D reconstruction for a resource-constrained Autonomous Underwater Vehicle (AUV). Underwater vision-guided operations are among the most challenging as they combine 3D motion in the presence of external forces, limited visibility, and absence of global positioning. Obstacle avoidance and effective path planning require online dense reconstructions of the environment. Autonomous operation is central to environmental monitoring, marine archaeology, resource utilization, and underwater cave exploration. To address this problem, we propose to use SVIn2, a robust VIO method, together with a real-time 3D reconstruction pipeline. We provide extensive evaluation on four challenging underwater datasets. Our pipeline produces comparable reconstruction with that of COLMAP, the state-of-the-art offline 3D reconstruction method, at high frame rates on a single CPU.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  doi           = {10.48550/ARXIV.2304.02704},
  eprint        = {2304.02704},
  file          = {:http\://arxiv.org/pdf/2304.02704v1:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@PhdThesis{Luczynski2018,
  author = {Tomasz Łuczyński},
  title  = {An Intelligent and Robust System for Underwater Vision},
  year   = {2018},
  groups = {Optics},
  url    = {http://nbn-resolving.org/urn:nbn:de:gbv:579-opus-1007798},
}

@PhdThesis{Lee2017,
  author   = {Lee, Yeejin},
  school   = {University of California, San Diego},
  title    = {Analysis of Raw Sensor Data with Applications in Image Processing and Compression},
  year     = {2017},
  type     = {phdthesis},
  abstract = {In the last few years, there has been a drastic improvement in the development of sensor technology that increases spatial resolution, dynamic range, and low-light sensitivity of digital cameras. Although the image processing techniques in a digital camera processing pipeline have been well studied, they face many difficulties in processing raw data of high resolution as well as raw data captured in the low light environment.

The key to maximizing the quality of the final output images is to understand the captured raw sensor data in the proper context of a camera processing pipeline. In this dissertation, we analyze the image acquisition model and develop new image processing techniques that leverage the acquisition model. This work advances computer vision and data communication by focusing on the role that the camera processing pipeline plays. Particularly, we discuss image enhancement in low-light condition and image compression problems in the context of the image pipeline.

To the best of our knowledge, this is the first study connecting image enhancement and compression algorithms to the context of an actual image acquisition process. Most prior image enhancement and image compression for raw sensor data used images already processed by camera processing pipelines for experimental verification while we verify the proposed image processing technique using actual sensor data.},
  groups   = {Image Processing},
  url      = {https://escholarship.org/uc/item/4063t2j4},
}

@Article{Amitai2022,
  author        = {Amitai, Shlomi and Klein, Itzik and Treibitz, Tali},
  title         = {Self-Supervised Monocular Depth Underwater},
  year          = {2022},
  month         = oct,
  abstract      = {Depth estimation is critical for any robotic system. In the past years estimation of depth from monocular images have shown great improvement, however, in the underwater environment results are still lagging behind due to appearance changes caused by the medium. So far little effort has been invested on overcoming this. Moreover, underwater, there are more limitations for using high resolution depth sensors, this makes generating ground truth for learning methods another enormous obstacle. So far unsupervised methods that tried to solve this have achieved very limited success as they relied on domain transfer from dataset in air. We suggest training using subsequent frames self-supervised by a reprojection loss, as was demonstrated successfully above water. We suggest several additions to the self-supervised framework to cope with the underwater environment and achieve state-of-the-art results on a challenging forward-looking underwater dataset.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  doi           = {10.48550/ARXIV.2210.03206},
  eprint        = {2210.03206},
  file          = {:http\://arxiv.org/pdf/2210.03206v1:PDF},
  groups        = {Monocular Depth Camera},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Book{Gonzalez,
  author    = {Rafael C. Gonzalez, Richard E. Woods},
  publisher = {Pearson},
  title     = {Digital Image Processing},
  year      = {2018},
  groups    = {Image Processing},
}

@Book{Leibe,
  author    = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  publisher = {Springer},
  title     = {Computer Vision – ECCV 2016 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I},
  isbn      = {9783319464473},
  groups    = {Image Processing},
  pages     = {902},
  subtitle  = {14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I},
}

@Article{Karaimer,
  author = {Hakki Can Karaimer, Michael S. Brown},
  title  = {A Software Platform for Manipulating the Camera Imaging Pipeline},
  doi    = {https://doi.org/10.1007/978-3-319-46448-0_26},
  groups = {Image Processing},
}

@Article{nguyen1995simple,
  author  = {Nguyen, Hoa G and Blackburn, Michael R},
  journal = {Technical Document},
  title   = {A simple method for range finding via laser triangulation},
  year    = {1995},
  volume  = {2734},
  groups  = {Laser Rangefinder},
}

@Article{Barreto2013,
  author  = {Saulo Vinicius Ferreira Barreto and Remy Eskinazi Sant'Anna and Marc{\'i}lio Andr{\'e} F{\'e}lix Feitosa},
  journal = {2013 IEEE 20th International Conference on Electronics, Circuits, and Systems (ICECS)},
  title   = {A method for image processing and distance measuring based on laser distance triangulation},
  year    = {2013},
  pages   = {695-698},
  groups  = {Laser Rangefinder},
  url     = {https://api.semanticscholar.org/CorpusID:17466529},
}

@Article{harvey2002estimation,
  author    = {Harvey, Euan and Fletcher, David and Shortis, Mark},
  journal   = {Fisheries Research},
  title     = {Estimation of reef fish length by divers and by stereo-video: a first comparison of the accuracy and precision in the field on living fish under operational conditions},
  year      = {2002},
  number    = {3},
  pages     = {255--265},
  volume    = {57},
  groups    = {Fish Analysis},
  publisher = {Elsevier},
}

@Article{10.1093/icesjms/fsx007,
  author   = {Shafait, Faisal and Harvey, Euan S. and Shortis, Mark R. and Mian, Ajmal and Ravanbakhsh, Mehdi and Seager, James W. and Culverhouse, Philip F. and Cline, Danelle E. and Edgington, Duane R.},
  journal  = {ICES Journal of Marine Science},
  title    = {{Towards automating underwater measurement of fish length: a comparison of semi-automatic and manual stereo–video measurements}},
  year     = {2017},
  issn     = {1054-3139},
  month    = {02},
  number   = {6},
  pages    = {1690-1701},
  volume   = {74},
  abstract = {Underwater stereo–video systems are widely used for counting and measuring fish in aquaculture, fisheries, and conservation management. Length measurements are generated from stereo–video recordings by a software operator using a mouse to locate the head and tail of a fish in synchronized pairs of images. This data can be used to compare spatial and temporal changes in the mean length and biomass or frequency distributions of populations of fishes. Since the early 1990s stereo–video has also been used for measuring the lengths of fish in aquaculture for quota and farm management. However, the costs of the equipment, software, the time, and salary costs involved in post processing imagery manually and the subsequent delays in the availability of length information inhibit the adoption of this technology.We present a semi-automatic method for capturing stereo–video measurements to estimate the lengths of fish. We compare the time taken to make measurements of the same fish measured manually from stereo–video imagery to that measured semi-automatically. Using imagery recorded during transfers of Southern Bluefin Tuna (SBT) from tow cages to grow out cages, we demonstrate that the semi-automatic algorithm developed can obtain fork length measurements with an error of less than 1\\% of the true length and with at least a sixfold reduction in operator time in comparison to manual measurements. Of the 22 138 SBT recorded we were able to measure 52.6\\% (11 647) manually and 11.8\\% (2614) semi-automatically. For seven of the eight cage transfers recorde,d there were no statistical differences in the mean length, weight, or length frequency between manual and semi-automatic measurements. When the data were pooled across the eight cage transfers, there was no statistical difference in mean length or weight between the stereo–video-based manual and semi-automated measurements. Hence, the presented semi-automatic system can be deployed to significantly reduce the cost involved in adoption of stereo–video technology.},
  doi      = {10.1093/icesjms/fsx007},
  eprint   = {https://academic.oup.com/icesjms/article-pdf/74/6/1690/31246707/fsx007.pdf},
  groups   = {Fish Analysis, Stereo Camera},
  url      = {https://doi.org/10.1093/icesjms/fsx007},
}

@Article{bell1985estimating,
  author    = {Bell, JD and Craik, GJS and Pollard, DA and Russell, BC},
  journal   = {Coral Reefs},
  title     = {Estimating length frequency distributions of large reef fish underwater},
  year      = {1985},
  pages     = {41--44},
  volume    = {4},
  doi       = {https://doi.org/10.1007/BF00302203},
  groups    = {Fish Analysis},
  publisher = {Springer},
}

@Article{caldwell2016,
  author    = {Caldwell, Zachary R. AND Zgliczynski, Brian J. AND Williams, Gareth J. AND Sandin, Stuart A.},
  journal   = {PLOS ONE},
  title     = {Reef Fish Survey Techniques: Assessing the Potential for Standardizing Methodologies},
  year      = {2016},
  month     = {04},
  number    = {4},
  pages     = {1-14},
  volume    = {11},
  abstract  = {Dramatic changes in populations of fishes living on coral reefs have been documented globally and, in response, the research community has initiated efforts to assess and monitor reef fish assemblages. A variety of visual census techniques are employed, however results are often incomparable due to differential methodological performance. Although comparability of data may promote improved assessment of fish populations, and thus management of often critically important nearshore fisheries, to date no standardized and agreed-upon survey method has emerged. This study describes the use of methods across the research community and identifies potential drivers of method selection. An online survey was distributed to researchers from academic, governmental, and non-governmental organizations internationally. Although many methods were identified, 89% of survey-based projects employed one of three methods - belt transect, stationary point count, and some variation of the timed swim method. The selection of survey method was independent of the research design (i.e., assessment goal) and region of study, but was related to the researchers home institution. While some researchers expressed willingness to modify their current survey protocols to more standardized protocols (76%), their willingness decreased when methodologies were tied to long-term datasets spanning five or more years. Willingness to modify current methodologies was also less common among academic researchers than resource managers. By understanding both the current application of methods and the reported motivations for method selection, we hope to focus discussions towards increasing the comparability of quantitative reef fish survey data.},
  doi       = {10.1371/journal.pone.0153066},
  groups    = {Fish Analysis},
  publisher = {Public Library of Science},
  url       = {https://doi.org/10.1371/journal.pone.0153066},
}

@Book{alma991004472629706535,
  author    = {Jennings, Simon and Kaiser, Michel J. and Reynolds, John D.},
  publisher = {Blackwell Science},
  title     = {Marine fisheries ecology},
  year      = {2001},
  address   = {Oxford ;},
  isbn      = {9781444311358},
  abstract  = {"This textbook describes fisheries exploitation, biology, conservation and management, and reflects many recent and important changes in fisheries science. These include growing concerns about the environmental impacts of fisheries, the role of ecological interactions in determining population dynamics, and the incorporation of uncertainty and precautionary principles into management advice. The book draws upon examples from tropical, temperate and polar environments, and provides readers with a broad understanding of the biological, economic and social aspects of fisheries ecology and the interplay between them. As well as covering 'classical' fisheries science, the book focuses on contemporary issues such as industrial fishing, poverty and conflict in fishing communities, marine reserves, the effects of fishing on coral reefs and by-catches of mammals, seabirds and reptiles. The book is primarily written for students of fisheries science and marine ecology, but should also appeal to practising fisheries scientists and those interested in conservation and the impacts of humans on the marine environment."--BOOK JACKET.},
  booktitle = {Marine fisheries ecology},
  groups    = {Fish Analysis},
  keywords  = {Ecologie marine.},
  language  = {eng},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Camera Calibration\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Fish Analysis\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Image Processing\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Laser Rangefinder\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Monocular Depth Camera\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Optics\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:RealSense System\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Stereo Camera\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Underwater Positioning\;0\;1\;0x8a8a8aff\;\;\;;
}
